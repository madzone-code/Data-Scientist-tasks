{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a4d8f6e",
   "metadata": {},
   "source": [
    "# Первая задача."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c4cd4a",
   "metadata": {},
   "source": [
    "2 способа распознавания:\n",
    "- *нейросеть whisper от OpenAI (требует установленного ffmpeg)*\n",
    "- *speech_recognition от google. ffmpeg не нужен.*\n",
    "\n",
    "Звуковой файл подготавливаем 2 способами:\n",
    "- *удаляем шум, нормализуем громкость*\n",
    "- *удаляем шум, выделяем голос, нормализуем громкость (метод тяжелый)*\n",
    "\n",
    "На выходе получаем 6 вариантов распознанного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f3a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Collecting moviepy\n",
      "  Using cached moviepy-2.1.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting noisereduce\n",
      "  Using cached noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting speechrecognition\n",
      "  Downloading speechrecognition-3.14.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting whisper\n",
      "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: decorator<6.0,>=4.0.2 in c:\\users\\максим\\appdata\\roaming\\python\\python313\\site-packages (from moviepy) (5.2.1)\n",
      "Collecting imageio<3.0,>=2.5 (from moviepy)\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
      "  Using cached imageio_ffmpeg-0.6.0-py3-none-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting numpy>=1.25.0 (from moviepy)\n",
      "  Using cached numpy-2.2.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting proglog<=1.0.0 (from moviepy)\n",
      "  Downloading proglog-0.1.11-py3-none-any.whl.metadata (794 bytes)\n",
      "Collecting python-dotenv>=0.10 (from moviepy)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pillow<11.0,>=9.2.0 (from moviepy)\n",
      "  Downloading pillow-10.4.0-cp313-cp313-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.61.2-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting scipy>=1.6.0 (from librosa)\n",
      "  Using cached scipy-1.15.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa)\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting joblib>=1.0 (from librosa)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting typing_extensions>=4.1.1 (from librosa)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Using cached msgpack-1.1.0-cp313-cp313-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting standard-aifc (from librosa)\n",
      "  Downloading standard_aifc-3.13.0-py3-none-any.whl.metadata (969 bytes)\n",
      "Collecting standard-sunau (from librosa)\n",
      "  Downloading standard_sunau-3.13.0-py3-none-any.whl.metadata (914 bytes)\n",
      "Collecting matplotlib (from noisereduce)\n",
      "  Using cached matplotlib-3.10.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting tqdm (from noisereduce)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting cffi>=1.0 (from soundfile)\n",
      "  Using cached cffi-1.17.1-cp313-cp313-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting audioop-lts (from speechrecognition)\n",
      "  Using cached audioop_lts-0.2.1-cp313-abi3-win_amd64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: six in c:\\users\\максим\\appdata\\roaming\\python\\python313\\site-packages (from whisper) (1.17.0)\n",
      "Collecting pycparser (from cffi>=1.0->soundfile)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: packaging in c:\\users\\максим\\appdata\\roaming\\python\\python313\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.44.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\максим\\appdata\\roaming\\python\\python313\\site-packages (from pooch>=1.1->librosa) (4.3.7)\n",
      "Collecting requests>=2.19.0 (from pooch>=1.1->librosa)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->noisereduce)\n",
      "  Using cached contourpy-1.3.1-cp313-cp313-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->noisereduce)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->noisereduce)\n",
      "  Using cached fonttools-4.57.0-cp313-cp313-win_amd64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->noisereduce)\n",
      "  Using cached kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->noisereduce)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\максим\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
      "Collecting standard-chunk (from standard-aifc->librosa)\n",
      "  Downloading standard_chunk-3.13.0-py3-none-any.whl.metadata (860 bytes)\n",
      "Requirement already satisfied: colorama in c:\\users\\максим\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->noisereduce) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.19.0->pooch>=1.1->librosa)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading moviepy-2.1.2-py3-none-any.whl (126 kB)\n",
      "Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/1.0 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading speechrecognition-3.14.2-py3-none-any.whl (32.9 MB)\n",
      "   ---------------------------------------- 0.0/32.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/32.9 MB 11.4 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 3.4/32.9 MB 10.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 5.0/32.9 MB 7.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 6.8/32.9 MB 8.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 8.7/32.9 MB 8.1 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 10.0/32.9 MB 7.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 11.8/32.9 MB 7.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 13.9/32.9 MB 8.2 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 16.3/32.9 MB 8.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 18.1/32.9 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 19.7/32.9 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 21.0/32.9 MB 8.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 22.8/32.9 MB 8.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 24.4/32.9 MB 8.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 26.2/32.9 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 27.8/32.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 29.4/32.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 31.2/32.9 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  32.5/32.9 MB 8.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 32.9/32.9 MB 7.8 MB/s eta 0:00:00\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading cffi-1.17.1-cp313-cp313-win_amd64.whl (182 kB)\n",
      "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Downloading imageio_ffmpeg-0.6.0-py3-none-win_amd64.whl (31.2 MB)\n",
      "   ---------------------------------------- 0.0/31.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.8/31.2 MB 8.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.9/31.2 MB 9.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 5.5/31.2 MB 8.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 7.3/31.2 MB 8.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 8.7/31.2 MB 8.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 10.5/31.2 MB 8.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 12.3/31.2 MB 8.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 14.2/31.2 MB 8.2 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 16.0/31.2 MB 8.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 17.8/31.2 MB 8.2 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 19.4/31.2 MB 8.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 21.2/31.2 MB 8.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 22.8/31.2 MB 8.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 24.9/31.2 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 26.2/31.2 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 28.6/31.2 MB 8.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 30.4/31.2 MB 8.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 31.2/31.2 MB 8.1 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.0-cp313-cp313-win_amd64.whl (75 kB)\n",
      "Downloading numba-0.61.2-cp313-cp313-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.8/2.8 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 8.3 MB/s eta 0:00:00\n",
      "Using cached numpy-2.2.4-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "Downloading pillow-10.4.0-cp313-cp313-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.8/2.6 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 8.6 MB/s eta 0:00:00\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading proglog-0.1.11-py3-none-any.whl (7.8 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.1/11.1 MB 10.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.9/11.1 MB 10.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.8/11.1 MB 9.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.1 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 8.6 MB/s eta 0:00:00\n",
      "Using cached scipy-1.15.2-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl (164 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached audioop_lts-0.2.1-cp313-abi3-win_amd64.whl (30 kB)\n",
      "Using cached matplotlib-3.10.1-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "Downloading standard_aifc-3.13.0-py3-none-any.whl (10 kB)\n",
      "Downloading standard_sunau-3.13.0-py3-none-any.whl (7.4 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached contourpy-1.3.1-cp313-cp313-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.57.0-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Downloading llvmlite-0.44.0-cp313-cp313-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.1/30.3 MB 9.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 3.9/30.3 MB 9.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 5.8/30.3 MB 8.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 7.6/30.3 MB 9.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 9.7/30.3 MB 8.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 11.0/30.3 MB 8.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 13.1/30.3 MB 8.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 15.2/30.3 MB 8.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 16.8/30.3 MB 8.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 18.4/30.3 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 19.7/30.3 MB 8.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 22.0/30.3 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 24.1/30.3 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 25.4/30.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 27.8/30.3 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  29.6/30.3 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 8.4 MB/s eta 0:00:00\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Downloading standard_chunk-3.13.0-py3-none-any.whl (4.9 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp313-cp313-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Building wheels for collected packages: whisper\n",
      "  Building wheel for whisper (pyproject.toml): started\n",
      "  Building wheel for whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41475 sha256=545c701dd4427d00088901ec366aa034ad8e792ed1daafaac28deb9ff5ec0490\n",
      "  Stored in directory: c:\\users\\максим\\appdata\\local\\pip\\cache\\wheels\\7e\\1e\\f0\\d36b92489c74925c5aa1aeb01d30f39ba018d2a1914e79ac36\n",
      "Successfully built whisper\n",
      "Installing collected packages: standard-chunk, whisper, urllib3, typing_extensions, tqdm, threadpoolctl, python-dotenv, pyparsing, pycparser, pillow, numpy, msgpack, llvmlite, lazy_loader, kiwisolver, joblib, imageio_ffmpeg, idna, fonttools, cycler, charset-normalizer, certifi, audioread, audioop-lts, standard-sunau, standard-aifc, soxr, scipy, requests, proglog, numba, imageio, contourpy, cffi, speechrecognition, soundfile, scikit-learn, pooch, moviepy, matplotlib, noisereduce, librosa\n",
      "Successfully installed audioop-lts-0.2.1 audioread-3.0.1 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.1 contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 idna-3.10 imageio-2.37.0 imageio_ffmpeg-0.6.0 joblib-1.4.2 kiwisolver-1.4.8 lazy_loader-0.4 librosa-0.11.0 llvmlite-0.44.0 matplotlib-3.10.1 moviepy-2.1.2 msgpack-1.1.0 noisereduce-3.0.3 numba-0.61.2 numpy-2.2.4 pillow-10.4.0 pooch-1.8.2 proglog-0.1.11 pycparser-2.22 pyparsing-3.2.3 python-dotenv-1.1.0 requests-2.32.3 scikit-learn-1.6.1 scipy-1.15.2 soundfile-0.13.1 soxr-0.5.0.post1 speechrecognition-3.14.2 standard-aifc-3.13.0 standard-chunk-3.13.0 standard-sunau-3.13.0 threadpoolctl-3.6.0 tqdm-4.67.1 typing_extensions-4.13.2 urllib3-2.4.0 whisper-1.1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/demucs.git 'C:\\Temp\\pip-req-build-j6pqq5d1'\n",
      "ERROR: Could not find a version that satisfies the requirement torchaudio<2.1,>=0.8 (from demucs) (from versions: 2.6.0)\n",
      "ERROR: No matching distribution found for torchaudio<2.1,>=0.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/demucs.git\n",
      "  Cloning https://github.com/facebookresearch/demucs.git to c:\\temp\\pip-req-build-j6pqq5d1\n",
      "  Resolved https://github.com/facebookresearch/demucs.git to commit e976d93ecc3865e5757426930257e200846a520a\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting dora-search (from demucs==4.1.0a2)\n",
      "  Using cached dora_search-0.1.12.tar.gz (87 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting einops (from demucs==4.1.0a2)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting julius>=0.2.3 (from demucs==4.1.0a2)\n",
      "  Using cached julius-0.2.7.tar.gz (59 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting lameenc>=1.2 (from demucs==4.1.0a2)\n",
      "  Using cached lameenc-1.8.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Collecting openunmix (from demucs==4.1.0a2)\n",
      "  Downloading openunmix-1.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting pyyaml (from demucs==4.1.0a2)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting torch>=1.8.1 (from demucs==4.1.0a2)\n",
      "  Using cached torch-2.6.0-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "INFO: pip is looking at multiple versions of demucs to determine which version is compatible with other requirements. This could take a while.\n",
      "ffmpeg уже установлен\n"
     ]
    }
   ],
   "source": [
    "# Установка необходимых библиотек\n",
    "%pip install moviepy librosa noisereduce soundfile SpeechRecognition openai-whisper demucs\n",
    "\n",
    "# Установка ffmpeg (для Windows/Linux/Mac) для whisper.\n",
    "try:\n",
    "    import subprocess\n",
    "    subprocess.run([\"ffmpeg\", \"-version\"], check=True, capture_output=True)\n",
    "    print(\"ffmpeg уже установлен\")\n",
    "except:\n",
    "    print(\"Установка ffmpeg...\")\n",
    "    import platform\n",
    "    if platform.system() == \"Windows\":\n",
    "        print(\"Для Windows установите ffmpeg вручную: https://ffmpeg.org/download.html\")\n",
    "    elif platform.system() == \"Linux\":\n",
    "        !apt-get update && apt-get install -y ffmpeg\n",
    "    elif platform.system() == \"Darwin\":  # MacOS\n",
    "        !brew install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb19322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем путь к аудиофайлу\n",
    "audio_path = \"task1/data/download_14.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0783483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import AudioFileClip\n",
    "import os\n",
    "import librosa\n",
    "import noisereduce as nr\n",
    "import shutil\n",
    "import soundfile as sf\n",
    "import speech_recognition as sr\n",
    "import subprocess\n",
    "import whisper\n",
    "\n",
    "\n",
    "def convertation(audio_path):\n",
    "    \"\"\"Конвертирование звукового файла в wav.\"\"\"\n",
    "\n",
    "    # Если уже wav, то не конвертируем.\n",
    "    if audio_path.lower().endswith('.wav'):\n",
    "        return audio_path\n",
    "\n",
    "    # Сохраняем файл по тому же адресу, но другим расширением.\n",
    "    wav_path = os.path.splitext(audio_path)[0] + '.wav'\n",
    "\n",
    "    audio = AudioFileClip(audio_path)\n",
    "    audio.write_audiofile(wav_path, codec='pcm_s16le')\n",
    "    audio.close()\n",
    "\n",
    "    print('Файл успешно конвертирован.')\n",
    "    return wav_path\n",
    "\n",
    "\n",
    "def enhance_audio(audio_path):\n",
    "    \"\"\"Базовая функция улучшения аудио. Убирает шумы, нормализует громкость.\"\"\"\n",
    "\n",
    "    # Загрузка аудио\n",
    "    audio_data, sample_rate = librosa.load(audio_path, sr=None)\n",
    "    # Удаление шума\n",
    "    reduced_noise = nr.reduce_noise(y=audio_data, sr=sample_rate)\n",
    "    # Нормализация громкости\n",
    "    normalized_audio = librosa.util.normalize(reduced_noise)\n",
    "    # Сохранение обработанного аудио\n",
    "    enhance_path = os.path.splitext(audio_path)[0] + '_enhance.wav'\n",
    "    sf.write(enhance_path, normalized_audio, sample_rate)\n",
    "\n",
    "    return enhance_path\n",
    "\n",
    "\n",
    "def advance_enhance_audio(wav_path):\n",
    "    \"\"\"\n",
    "    Продвинутая функция улучшения аудио.\n",
    "    Убирает шумы, оставляет один голос, нормализует громкость.\n",
    "\n",
    "    \"\"\"\n",
    "    # Выходной файл\n",
    "    advance_enhanced_path = (\n",
    "        os.path.splitext(wav_path)[0] + '_advance_enhance.wav')\n",
    "    # Загрузка аудио\n",
    "    audio, sample_rate = librosa.load(wav_path, sr=16000)  # 16 кГц для Whisper\n",
    "    # Легкое подавление шума\n",
    "    reduced_noise = nr.reduce_noise(y=audio, sr=sample_rate, stationary=False,\n",
    "                                    prop_decrease=0.75)\n",
    "    # Сохранение временного файла\n",
    "    temp_file = \"temp.wav\"\n",
    "    sf.write(temp_file, reduced_noise, sample_rate)\n",
    "    # Выделение голоса с Demucs.\n",
    "    subprocess.run([\"demucs\", \"--two-stems=vocals\", temp_file], check=True)\n",
    "    vocal_file = \"separated/htdemucs/temp/vocals.wav\"\n",
    "    # Нормализация громкости с ffmpeg\n",
    "    subprocess.run([\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", vocal_file,\n",
    "        \"-af\", \"volume=5dB\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-y\", advance_enhanced_path\n",
    "    ], check=True)\n",
    "\n",
    "    # Удаление временных файлов\n",
    "    os.remove(temp_file)\n",
    "    if os.path.exists(\"separated\"):\n",
    "        shutil.rmtree(\"separated\")\n",
    "\n",
    "    return advance_enhanced_path\n",
    "\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Локальное распознавание. Не требует ffmpeg.\"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    print('Подождите, идет распознавание файла.')\n",
    "\n",
    "    try:\n",
    "        with sr.AudioFile(audio_path) as source:\n",
    "            audio = recognizer.record(source)\n",
    "            text = recognizer.recognize_google(audio, language=\"ru-RU\")\n",
    "            return text\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Не удалось распознать речь\"\n",
    "    except sr.RequestError as e:\n",
    "        return f\"Ошибка сервиса: {e}\"\n",
    "\n",
    "\n",
    "def whisper_transcribe_audio(wav_path):\n",
    "    \"\"\"\n",
    "    Используем нейросетевую модель от OpenAI. Доступны модели tiny, base,\n",
    "    small, medium, large. Использует ffmpeg (должен быть установлен в ОС).\n",
    "    \"\"\"\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(wav_path, language=\"ru\")\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "# Обработка исходного файла.\n",
    "def main():\n",
    "    # Задаем путь к обрабатываемому аудио файлу.\n",
    "    audio_path = \"task1/data/download_16.mp4\"\n",
    "\n",
    "    # Конвертируем в wav.\n",
    "    wav_path = convertation(audio_path)\n",
    "\n",
    "    # Улучшаем wav 2 способами.\n",
    "    enhanced_wav_path = enhance_audio(wav_path)\n",
    "    advance_enhanced_wav_path = advance_enhance_audio(wav_path)\n",
    "\n",
    "    # Распознаем текст\n",
    "    texts = {\n",
    "        \"original\": transcribe_audio(wav_path),\n",
    "        \"enhanced\": transcribe_audio(enhanced_wav_path),\n",
    "        \"advance_enhanced\": transcribe_audio(advance_enhanced_wav_path),\n",
    "        \"whisper\": whisper_transcribe_audio(wav_path),\n",
    "        \"whisper_enhanced\": whisper_transcribe_audio(enhanced_wav_path),\n",
    "        \"whisper_advance_enhanced\": whisper_transcribe_audio(\n",
    "            advance_enhanced_wav_path),\n",
    "    }\n",
    "\n",
    "    # Записываем все тексты в файлы по тому же адресу, что и оригинал.\n",
    "    base_path = os.path.splitext(audio_path)[0]\n",
    "    for key, text in texts.items():\n",
    "        with open(f\"{base_path}_{key}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc524586",
   "metadata": {},
   "source": [
    "# Вторая задача."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e41c9",
   "metadata": {},
   "source": [
    "Используем нейросеть yolo11n для сегментации автомобилей.\n",
    "Маскируем стекла и колеса.\n",
    "Вычисляем средний цвет.\n",
    "Визуализируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых библиотек\n",
    "%pip install opencv-python numpy matplotlib ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e456a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем путь к фотографии автомобиля\n",
    "image_path = \"task2/data/istockphoto-494522913-612x612.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f525e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "def detect_cars(model, image_rgb):\n",
    "    \"\"\"Выполняет сегментацию автомобилей с помощью модели YOLO.\"\"\"\n",
    "    results = model(image_rgb)\n",
    "    cars = []\n",
    "    masks = []\n",
    "    for result in results:\n",
    "        for box, mask in zip(result.boxes, result.masks or []):\n",
    "            if int(box.cls) == 2:       # Класс 2 в COCO — автомобиль\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cars.append((x1, y1, x2, y2))\n",
    "                mask_data = mask.data.cpu().numpy().squeeze()\n",
    "                masks.append(mask_data)\n",
    "    return cars, masks\n",
    "\n",
    "\n",
    "def process_car(image_rgb, car_coords, mask_data):\n",
    "    \"\"\"\n",
    "    Обрабатывает один автомобиль:\n",
    "    сегментирует, исключает стёкла/колёса, вычисляет цвет.\n",
    "\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = car_coords\n",
    "    # Выделяем область автомобиля\n",
    "    car_region = image_rgb[y1:y2, x1:x2].copy()\n",
    "\n",
    "    # Изменение размера маски до размеров bounding box\n",
    "    mask_resized = cv2.resize(\n",
    "        mask_data, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "    mask_resized = (mask_resized > 0).astype(np.uint8) * 255\n",
    "\n",
    "    # Применение маски сегментации\n",
    "    car_segmented = cv2.bitwise_and(car_region, car_region, mask=mask_resized)\n",
    "\n",
    "    # Преобразование в HSV\n",
    "    car_hsv = cv2.cvtColor(car_segmented, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Маскирование стёкол и колёс\n",
    "    lower_glass = np.array([0, 0, 150])  # Высокая яркость\n",
    "    upper_glass = np.array([180, 50, 255])  # Низкая насыщенность\n",
    "    lower_wheels = np.array([0, 0, 0])  # Тёмные области\n",
    "    upper_wheels = np.array([180, 255, 50])  # Низкая яркость\n",
    "\n",
    "    mask_glass = cv2.inRange(car_hsv, lower_glass, upper_glass)\n",
    "    mask_wheels = cv2.inRange(car_hsv, lower_wheels, upper_wheels)\n",
    "    mask_exclude = cv2.bitwise_or(mask_glass, mask_wheels)\n",
    "    mask_body = cv2.bitwise_and(mask_resized, cv2.bitwise_not(mask_exclude))\n",
    "\n",
    "    # Применение финальной маски\n",
    "    car_body = cv2.bitwise_and(car_region, car_region, mask=mask_body)\n",
    "\n",
    "    # Вычисление среднего цвета\n",
    "    valid_pixels = car_body[mask_body > 0].reshape(-1, 3)\n",
    "    if len(valid_pixels) > 0:\n",
    "        mean_color = np.mean(valid_pixels, axis=0).astype(int)\n",
    "    else:\n",
    "        mean_color = np.array([128, 128, 128])\n",
    "\n",
    "    return tuple(mean_color)\n",
    "\n",
    "\n",
    "def visualize_results(image_rgb, cars, rgb_colors):\n",
    "    \"\"\"Визуализация.\"\"\"\n",
    "    # Копия изображения для рисования\n",
    "    vis_image = image_rgb.copy()\n",
    "\n",
    "    # Рисуем bounding box'ы и метки\n",
    "    for i, (x1, y1, x2, y2) in enumerate(cars):\n",
    "        cv2.rectangle(vis_image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(vis_image, f\"Car {i+1}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Визуализация\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Исходное изображение\n",
    "    plt.subplot(1, len(cars) + 1, 1)\n",
    "    plt.imshow(vis_image)\n",
    "    plt.title(\"Автомобили с определёнными цветами\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Цветовые патчи\n",
    "    for i, color in enumerate(rgb_colors):\n",
    "        plt.subplot(1, len(cars) + 1, i + 2)\n",
    "        color_patch = np.ones((100, 100, 3)) * (np.array(color) / 255)\n",
    "        plt.imshow(color_patch)\n",
    "        plt.title(f\"Авто {i+1}\\nRGB: {color}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Загрузка модели (vj;)\n",
    "    model = YOLO(\"yolo11n-seg.pt\")\n",
    "\n",
    "    # Подготовка изображения\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Детекция автомобилей\n",
    "    cars, masks = detect_cars(model, image_rgb)\n",
    "    if not cars:\n",
    "        print(\"Автомобили не найдены\")\n",
    "        return\n",
    "\n",
    "    # Обработка автомобилей\n",
    "    rgb_colors = []\n",
    "    for i, (car_coords, mask_data) in enumerate(zip(cars, masks)):\n",
    "        color = process_car(image_rgb, car_coords, mask_data)\n",
    "        rgb_colors.append(color)\n",
    "        print(f\"Авто {i+1}: RGB = {color}\")\n",
    "\n",
    "    # Визуализация результатов\n",
    "    visualize_results(image_rgb, cars, rgb_colors)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a05fc5",
   "metadata": {},
   "source": [
    "# Третья задача"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e4299",
   "metadata": {},
   "source": [
    "Для решения напрашивается нейронная сеть на базе сверточной основы с обучением на сгенерированных фейковых документах с аугментацией и k-fold валидацией. Проблем с реализацией не будет, но займет существенное время. Если необходимо убедиться, что я могу это сделать, посмотрите проект https://github.com/madzone-code/FaceGenderBot\n",
    "На мой взгляд, оптимальное решение в рамках тестового задания – на основе распознавания текста (писал для своего проекта). Да, оно не работает, если изображение очень низкого качества (часть предложенных паспортов). Однако, тут возникает вопрос: «а зачем такое изображение вообще классифицировать, если оно абсолютно бесполезно для дальнейшей обработки?». \n",
    "Плюс tesseract должен быть установлен в системе (прописан в PATH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fc1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Установка необходимых библиотек\n",
    "%pip install opencv-python pytesseract\n",
    "\n",
    "# Убедитесь, что tesseract установлен в системе и прописан в PATH\n",
    "# https://github.com/UB-Mannheim/tesseract/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Указываем путь к фотографии документа\n",
    "image_path = 'task3/data/$2y$10$k1fd1.d6HmhGzjzlTay.ChRiDY8LgriFg.EupH6kUCTCt9fjRm.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e374470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "# Указываем перечень контрольных (уникальных) слов для определения типа.\n",
    "DOCUMENTS_TYPE = {\n",
    "    'договор': ['договор',],\n",
    "    'паспорт': ['отделом', 'мвд'],\n",
    "    'СТС': ['certificat'],\n",
    "    'ИНН': ['налогам'],\n",
    "    'права': ['водительское'],\n",
    "}\n",
    "\n",
    "\n",
    "# Само распознавание.\n",
    "def ocr(file_path):\n",
    "    \"\"\"Принимаем путь к картинке и возвращаем сырые данные.\"\"\"\n",
    "    image = cv2.imread(file_path)\n",
    "    # Конвертация изображения в градации серого.\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Конфиг для лучшего распознавания.\n",
    "    custom_config = r'--oem 3 --psm 6'\n",
    "    # Распознаем.\n",
    "    text = pytesseract.image_to_string(\n",
    "        gray_image,\n",
    "        lang='rus+eng',\n",
    "        config=custom_config\n",
    "    )\n",
    "    return set(text.lower().split())         # множество для скорости работы.\n",
    "\n",
    "\n",
    "def define_type(raw_text):\n",
    "    for key, values in DOCUMENTS_TYPE.items():\n",
    "        for value in values:\n",
    "            if value in raw_text:\n",
    "                return key\n",
    "    return 'Не удалось распознать документ.'\n",
    "\n",
    "\n",
    "def main():\n",
    "    raw_text = ocr(image_path)\n",
    "    result = define_type(raw_text)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d417d56",
   "metadata": {},
   "source": [
    "# Шестая задача"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1663fb6c",
   "metadata": {},
   "source": [
    "За основу берем решение 2 задачи. Добавляем веб-интерфейс, добавляем кеширование. Развертываем в контейнере Докер, выгружаем на докерхаб.\n",
    "  \n",
    "**Для развертывания контейнера выполните:**  \n",
    "*docker pull madzonedocker/car-color-detection:latest*  \n",
    "*docker run -p 7860:7860 --name car-color-container madzonedocker/car-color-detection:latest*  \n",
    "**Для проверки перейдите по ссылке:**  \n",
    "*http://localhost:7860*  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
